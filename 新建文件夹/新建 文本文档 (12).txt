下面给出**两个任务**各自的**完整任务卡**（JSON样式），每张任务卡都包含：建模方案、**可直接运行**的 Python 代码（内置合成数据以便即刻跑通；替换为真实 SQL/数据后即可落地）、评估与指标门槛、业务落地方案（含 ROI/预算约束下的最优召回名单生成）、以及持续优化与监控指引。

> 说明：代码默认使用 `pandas/numpy/scikit-learn/matplotlib`，并优先尝试 `xgboost`/`lightgbm`（若环境未安装会自动退化为 `GradientBoostingClassifier`）。代码会生成以下文件：
>
> * `model.pkl`（模型）
> * `feature_importance.png`、`roc_curve.png`、`pr_curve.png`、`confusion_matrix.png`
> * `churn_users.csv`（召回清单，含策略与预算）
> * `evaluation_report.md`（评估报告）
> * A/B 框架会生成：`experiment_report.md`

---

````json
{
  "id": "DATA-SCIENCE-001",
  "title": "用户流失预测建模与召回策略优化",
  "objective": "量化14天内流失风险，输出受预算约束的最优召回清单与策略，并达到可上线的评估门槛",
  "建模方案": {
    "问题定义": "二分类：在观测窗（最近30天）后、预测窗（未来14天）是否发生流失（无登录/无任务/无支付）",
    "标签定义": "在快照日t之后的[t+1, t+14]无活跃即标记为流失=1，否则=0（以时间滚动构造样本，严格避免泄漏）",
    "特征工程": [
      "行为频率：最近30天活跃天数、登录次数、任务创建数、完成数、失败率、平均处理时长、p95处理时长",
      "价值相关：最近30天付费次数、付费金额、客单价、最近一次付费间隔（天）、优惠券使用次数/金额",
      "粘性/多样性：功能使用种类数、近30天周末占比、夜间使用占比、平均会话间隔",
      "RFM：最近一次活跃间隔R、近30天活跃频次F、近90天金额M及分位标准化",
      "属性：注册渠道（one-hot topK）、设备类型（Web/iOS/Android/桌面）、地域（省/国别topK+other）",
      "配额相关：配额消耗率、配额用尽次数、用尽后第二天留存标记（历史）"
    ],
    "算法选择": "优先 XGBoost/LightGBM（非线性、缺失鲁棒、特征重要度清晰）；同时训练对比：Logistic、RandomForest，择优以AUC为准",
    "评估指标": "主指标：ROC-AUC ≥ 0.70；辅指标：PR-AUC；在预算与ROI约束下选择阈值，使Precision ≥ 0.60 且预期ROI > 0",
    "训练集/测试集": "按时间切分：最近7天样本作为测试集，其余训练；可加入滚动验证（time-based CV）",
    "阈值与分群": "默认阈值用于报告（如F0.5最优或Youden J）；业务分群：高风险p≥0.70，中风险0.40≤p<0.70，低风险p<0.40；随后在预算约束下以期望利润/成本排序进行人群选择与策略分配"
  },
  "技术实现": {
    "Python代码": "```python\n# -*- coding: utf-8 -*-\n\"\"\"\n用户流失预测与召回策略（可直接运行）\n- 若无真实数据，脚本会生成可控的合成样本以跑通全流程\n- 生成输出：model.pkl、feature_importance.png、roc_curve.png、pr_curve.png、confusion_matrix.png、\n            churn_users.csv、evaluation_report.md\n依赖：pandas numpy scikit-learn matplotlib；优先尝试 xgboost / lightgbm；若无则自动回退为 GradientBoostingClassifier\n\"\"\"\nimport os, json, math, warnings, pickle\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n# ML\nfrom sklearn.model_selection import train_test_split, TimeSeriesSplit\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import roc_auc_score, precision_recall_curve, average_precision_score, roc_curve, confusion_matrix, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.inspection import permutation_importance\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n\n# 优先导入 GBDT 库\nHAVE_XGB = False\nHAVE_LGBM = False\ntry:\n    import xgboost as xgb\n    HAVE_XGB = True\nexcept Exception:\n    pass\ntry:\n    import lightgbm as lgb\n    HAVE_LGBM = True\nexcept Exception:\n    pass\n\n# ----------------------- 配置 -----------------------\nCFG = {\n    'seed': 42,\n    'observation_days': 30,   # 特征观测窗\n    'horizon_days': 14,       # 预测窗（定义流失）\n    'test_days': 7,           # 按时间最近7天做测试集\n    'monthly_budget': 5000.0, # 召回月预算（RMB）\n    'costs': {'coupon': 5.0, 'sms': 0.05, 'push': 0.0}, # 单人触达成本\n    'recall_rate': {'high': 0.30, 'medium': 0.10, 'low': 0.03}, # 触达后召回成功率假设\n    'precision_floor': 0.60,  # 业务精度下限\n    'shrinkage_alpha': 0.60,  # 预计可回收收入=alpha*近30天收入\n}\nnp.random.seed(CFG['seed'])\n\n# ----------------------- 数据准备（合成可运行；替换成真实SQL导出即可） -----------------------\n\ndef synthesize_user_table(n=30000):\n    # 合成用户日快照（每用户一行）\n    uid = np.arange(1, n+1)\n    snapshot_start = datetime(2025,9,1)\n    snap_dates = np.array([snapshot_start + timedelta(days=int(d)) for d in np.random.randint(0, 37, size=n)])\n    active_days_30 = np.clip(np.random.poisson(10, n), 0, 30)\n    login_cnt_30 = active_days_30 + np.random.poisson(5, n)\n    tasks_created_30 = np.clip(np.random.poisson(12, n), 0, None)\n    success_rate = np.clip(np.random.beta(7,3, n), 0, 1)\n    fail_rate = 1 - success_rate + np.random.normal(0, 0.02, n)\n    fail_rate = np.clip(fail_rate, 0, 1)\n    p95_proc_ms = np.clip(np.random.normal(2200, 500, n), 500, None)\n    avg_proc_ms = p95_proc_ms / np.random.uniform(2.2, 3.2, n)\n    pay_cnt_30 = np.clip(np.random.poisson(0.8* (1+active_days_30/30), n), 0, None)\n    pay_amt_30 = np.round(np.random.gamma(2.0, 10.0, n) * (1+pay_cnt_30*0.3), 2)\n    aov = np.where(pay_cnt_30>0, pay_amt_30/np.maximum(1,pay_cnt_30), 0)\n    last_pay_gap = np.where(pay_cnt_30>0, np.random.randint(0, 45, n), 999)\n    coupon_used_cnt_30 = np.where(pay_cnt_30>0, np.random.binomial(pay_cnt_30, 0.25), 0)\n    feature_variety = np.clip(np.random.poisson(4, n)+1, 1, 15)\n    weekend_share = np.clip(np.random.normal(0.28, 0.1, n), 0, 1)\n    night_share = np.clip(np.random.normal(0.22, 0.1, n), 0, 1)\n    avg_session_gap_h = np.clip(np.random.normal(36, 12, n), 1, None)\n    quota_use_ratio = np.clip(np.random.beta(2,3, n), 0, 1)\n\n    channels = np.random.choice(['ads_tiktok', 'ads_fb', 'seo', 'direct', 'referral'], size=n, p=[0.18,0.22,0.26,0.22,0.12])\n    devices = np.random.choice(['web','ios','android','desktop'], size=n, p=[0.55,0.15,0.25,0.05])\n    regions = np.random.choice(['CN','US','IN','EU','BR','Other'], size=n, p=[0.32,0.24,0.16,0.14,0.08,0.06])\n\n    # 真实逻辑：更多活跃、更多付费、成功率高 => 更不容易流失\n    logit = (\n        1.0\n        - 0.09*active_days_30\n        - 0.25*pay_cnt_30\n        - 1.2*success_rate\n        + 0.8*fail_rate\n        + 0.002*(p95_proc_ms-2200)\n        + 0.5*(last_pay_gap>30)\n        - 0.05*feature_variety\n        + 0.15*avg_session_gap_h/24.0\n        + 0.4*(quota_use_ratio<0.1)\n    )\n    # 渠道/设备弱影响\n    logit += np.where(channels=='ads_fb', 0.05, 0) + np.where(devices=='ios', -0.05, 0)\n    p_churn = 1 / (1 + np.exp(-logit))\n    y = np.random.binomial(1, np.clip(p_churn, 0.01, 0.99))\n\n    df = pd.DataFrame({\n        'user_id': uid,\n        'snapshot_date': snap_dates,\n        'active_days_30': active_days_30,\n        'login_cnt_30': login_cnt_30,\n        'tasks_created_30': tasks_created_30,\n        'success_rate': success_rate,\n        'fail_rate': fail_rate,\n        'avg_proc_ms': avg_proc_ms,\n        'p95_proc_ms': p95_proc_ms,\n        'pay_cnt_30': pay_cnt_30,\n        'pay_amt_30': pay_amt_30,\n        'aov': aov,\n        'last_pay_gap': last_pay_gap,\n        'coupon_used_cnt_30': coupon_used_cnt_30,\n        'feature_variety': feature_variety,\n        'weekend_share': weekend_share,\n        'night_share': night_share,\n        'avg_session_gap_h': avg_session_gap_h,\n        'quota_use_ratio': quota_use_ratio,\n        'channel': channels,\n        'device': devices,\n        'region': regions,\n        'label_churn_14d': y,\n    })\n    return df\n\n# ----------------------- 训练/评估 -----------------------\n\ndef build_models():\n    models = {}\n    # 逻辑回归（带标准化）\n    models['logreg'] = (\n        LogisticRegression(max_iter=200, n_jobs=None, class_weight='balanced', solver='lbfgs')\n    )\n    # 随机森林\n    models['rf'] = RandomForestClassifier(n_estimators=300, max_depth=12, min_samples_leaf=10, n_jobs=-1, random_state=CFG['seed'])\n    # XGB/LGBM 优先\n    if HAVE_XGB:\n        models['xgb'] = xgb.XGBClassifier(\n            n_estimators=400, max_depth=6, learning_rate=0.05, subsample=0.9, colsample_bytree=0.9,\n            reg_lambda=1.0, random_state=CFG['seed'], n_jobs=-1, eval_metric='logloss'\n        )\n    if HAVE_LGBM:\n        models['lgbm'] = lgb.LGBMClassifier(\n            n_estimators=600, max_depth=-1, num_leaves=63, learning_rate=0.05, subsample=0.9, colsample_bytree=0.9,\n            reg_lambda=1.0, random_state=CFG['seed'], n_jobs=-1\n        )\n    # 回退 GBDT\n    if not (HAVE_XGB or HAVE_LGBM):\n        models['gbrt'] = GradientBoostingClassifier(random_state=CFG['seed'])\n    return models\n\n\ndef train_and_evaluate(df):\n    # 按时间切分\n    cutoff = df['snapshot_date'].max() - timedelta(days=CFG['test_days'])\n    df_train = df[df['snapshot_date'] <= cutoff].copy()\n    df_test = df[df['snapshot_date'] > cutoff].copy()\n\n    y_train = df_train['label_churn_14d'].values\n    y_test = df_test['label_churn_14d'].values\n\n    num_cols = [\n        'active_days_30','login_cnt_30','tasks_created_30','success_rate','fail_rate',\n        'avg_proc_ms','p95_proc_ms','pay_cnt_30','pay_amt_30','aov','last_pay_gap',\n        'coupon_used_cnt_30','feature_variety','weekend_share','night_share','avg_session_gap_h','quota_use_ratio'\n    ]\n    cat_cols = ['channel','device','region']\n\n    # 预处理：one-hot 类别；LogReg 再叠加标准化\n    pre_num = StandardScaler(with_mean=True, with_std=True)\n    pre_cat = OneHotEncoder(handle_unknown='infrequent_if_exist', min_frequency=50, sparse_output=False)\n\n    X_train_num = df_train[num_cols]\n    X_test_num = df_test[num_cols]\n    X_train_cat = df_train[cat_cols]\n    X_test_cat = df_test[cat_cols]\n\n    ohe = pre_cat.fit(X_train_cat)\n    Xtr_cat = ohe.transform(X_train_cat)\n    Xte_cat = ohe.transform(X_test_cat)\n    Xtr = np.hstack([X_train_num.values, Xtr_cat])\n    Xte = np.hstack([X_test_num.values, Xte_cat])\n    scaler = pre_num.fit(X_train_num)\n    Xtr[:, :len(num_cols)] = scaler.transform(X_train_num)\n    Xte[:, :len(num_cols)] = scaler.transform(X_test_num)\n\n    feature_names = num_cols + list(ohe.get_feature_names_out(cat_cols))\n\n    models = build_models()\n    results = {}\n\n    best_model_name, best_auc = None, -1\n    best_model = None\n    for name, mdl in models.items():\n        mdl.fit(Xtr, y_train)\n        proba = mdl.predict_proba(Xte)[:,1] if hasattr(mdl, 'predict_proba') else mdl.decision_function(Xte)\n        auc = roc_auc_score(y_test, proba)\n        ap = average_precision_score(y_test, proba)\n        results[name] = {'auc': auc, 'ap': ap, 'model': mdl}\n        if auc > best_auc:\n            best_auc, best_model, best_model_name = auc, mdl, name\n\n    # 画ROC/PR\n    proba_best = best_model.predict_proba(Xte)[:,1] if hasattr(best_model, 'predict_proba') else best_model.decision_function(Xte)\n    fpr, tpr, thr = roc_curve(y_test, proba_best)\n    plt.figure(); plt.plot(fpr, tpr); plt.plot([0,1],[0,1],'--'); plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title(f'ROC AUC={best_auc:.3f}'); plt.savefig('roc_curve.png', dpi=150); plt.close()\n    prec, rec, thr_pr = precision_recall_curve(y_test, proba_best)\n    ap_best = average_precision_score(y_test, proba_best)\n    plt.figure(); plt.plot(rec, prec); plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title(f'PR AUC={ap_best:.3f}'); plt.savefig('pr_curve.png', dpi=150); plt.close()\n\n    # 选择用于报告的阈值：F0.5 最大化（偏向 Precision），并满足 Precision 下限\n    beta = 0.5\n    fbeta = (1+beta**2) * (prec*rec) / (beta**2*prec + rec + 1e-9)\n    idx = np.nanargmax(fbeta)\n    th_report = thr_pr[max(idx-1,0)] if idx>0 else 0.5\n    # 若不达 Precision 下限，则提高阈值直到达标\n    def find_threshold_for_precision(prec, rec, thr, floor):\n        for p, t in zip(prec[::-1], np.append(thr, 1.0)[::-1]):\n            if p >= floor:\n                return t\n        return 0.5\n    th_prec_floor = find_threshold_for_precision(prec, rec, thr_pr, CFG['precision_floor'])\n    th = max(th_report, th_prec_floor)\n\n    y_pred = (proba_best >= th).astype(int)\n    cm = confusion_matrix(y_test, y_pred)\n    plt.figure();\n    plt.imshow(cm, cmap='Blues'); plt.title(f'Confusion Matrix @ {th:.3f}');\n    plt.xlabel('Predicted'); plt.ylabel('Actual');\n    for i in range(2):\n        for j in range(2):\n            plt.text(j, i, cm[i, j], ha='center', va='center')\n    plt.savefig('confusion_matrix.png', dpi=150); plt.close()\n\n    # 特征重要度（统一用 permutation importance，避免模型差异带来的不可比）\n    perm = permutation_importance(best_model, Xte, y_test, n_repeats=10, random_state=CFG['seed'], n_jobs=-1)\n    imp = pd.DataFrame({'feature': feature_names, 'importance': perm.importances_mean}).sort_values('importance', ascending=False)\n    plt.figure(figsize=(8,6))\n    topk = imp.head(20)[::-1]\n    plt.barh(topk['feature'], topk['importance'])\n    plt.tight_layout(); plt.savefig('feature_importance.png', dpi=150); plt.close()\n\n    # 保存模型与预处理器信息（便于线上一致）\n    with open('model.pkl', 'wb') as f:\n        pickle.dump({'model': best_model, 'scaler': scaler, 'ohe': ohe, 'num_cols': num_cols, 'cat_cols': cat_cols, 'feature_names': feature_names, 'threshold': th}, f)\n\n    eval_report = {\n        'best_model': best_model_name,\n        'auc': float(best_auc),\n        'ap': float(ap_best),\n        'threshold': float(th),\n        'confusion_matrix': cm.tolist(),\n        'classification_report': classification_report(y_test, y_pred, output_dict=True),\n        'top_features': imp.head(30).to_dict(orient='records')\n    }\n    return eval_report, best_model, (Xte, y_test, proba_best), df_test, imp\n\n# ----------------------- 预算约束下的召回优化 -----------------------\n\ndef assign_risk_and_action(p):\n    if p >= 0.70:\n        return 'high', 'coupon'\n    elif p >= 0.40:\n        return 'medium', 'sms'\n    else:\n        return 'low', 'push'\n\n\ndef optimize_under_budget(test_df, proba, value_col='pay_amt_30'):\n    # 预计可回收收入（保守）：alpha * 近30天收入；若无收入则用AOV或常数兜底\n    alpha = CFG['shrinkage_alpha']\n    est_value = alpha * np.where(test_df[value_col].values>0, test_df[value_col].values, np.median(test_df[value_col].values))\n\n    rows = []\n    for uid, p, v in zip(test_df['user_id'].values, proba, est_value):\n        tier, action = assign_risk_and_action(p)\n        recall = CFG['recall_rate'][tier]\n        cost = CFG['costs'][action]\n        exp_profit = p * recall * v - cost  # 粗略期望利润\n        roi = (exp_profit / cost) if cost>0 else (p*recall*v/0.01)  # push成本0，用0.01近似只用于排序展示\n        rows.append((uid, float(p), tier, action, float(cost), float(recall), float(v), float(exp_profit), float(roi)))\n\n    rec = pd.DataFrame(rows, columns=['user_id','churn_prob','risk_tier','action','cost','recall_rate','expected_value','expected_profit','roi_like'])\n\n    # 优先选择正期望利润，按 \"期望利润/成本\" 排序，在预算内选人\n    paid_mask = rec['action'].isin(['coupon','sms'])\n    paid = rec[paid_mask].copy()\n    paid = paid[paid['expected_profit']>0].sort_values('roi_like', ascending=False)\n\n    budget = CFG['monthly_budget']\n    cum_cost = paid['cost'].cumsum()\n    take = paid[cum_cost <= budget].copy()\n\n    # push 全量触达（不占预算）\n    push = rec[rec['action']=='push'].copy()\n\n    final = pd.concat([take, push], axis=0).sort_values(['action','churn_prob'], ascending=[True, False])\n    final['expected_roi'] = np.where(final['cost']>0, final['expected_profit']/final['cost'], np.nan)\n    final = final[['user_id','churn_prob','risk_tier','action','cost','recall_rate','expected_value','expected_profit','expected_roi']]\n\n    # 保存召回清单\n    final.to_csv('churn_users.csv', index=False)\n\n    # 汇总预算与回报\n    total_cost = take['cost'].sum()\n    total_profit = take['expected_profit'].sum()\n    return final, float(total_cost), float(total_profit)\n\n# ----------------------- 主流程 -----------------------\nif __name__ == '__main__':\n    df = synthesize_user_table(n=30000)\n    report, model, pack, df_test, fi = train_and_evaluate(df)\n    Xte, yte, pte = pack\n\n    # 预算优化与清单\n    final_list, used_cost, exp_profit = optimize_under_budget(df_test, pte, value_col='pay_amt_30')\n\n    # 写评估报告\n    lines = []\n    lines.append(f\"# 流失模型评估报告\\n\")\n    lines.append(f\"**最佳模型**: {report['best_model']}  \")\n    lines.append(f\"**测试AUC**: {report['auc']:.3f}  **PR-AUC**: {report['ap']:.3f}  **阈值**: {report['threshold']:.3f}\\n\")\n    cm = np.array(report['confusion_matrix'])\n    tn, fp, fn, tp = cm.ravel()\n    precision = tp/(tp+fp+1e-9); recall = tp/(tp+fn+1e-9)\n    lines.append(f\"**Precision**: {precision:.3f}  **Recall**: {recall:.3f}\\n\")\n    lines.append(f\"![](roc_curve.png)\\n\")\n    lines.append(f\"![](pr_curve.png)\\n\")\n    lines.append(f\"![](confusion_matrix.png)\\n\")\n    lines.append(f\"![](feature_importance.png)\\n\")\n    lines.append(f\"\\n## 预算与ROI\\n\")\n    lines.append(f\"选择付费触达人群成本合计：¥{used_cost:,.2f} / 预算¥{CFG['monthly_budget']:,.2f}  预期利润：¥{exp_profit:,.2f}\\n\")\n    lines.append(f\"召回清单：见 churn_users.csv（user_id, churn_prob, risk_tier, action, cost, expected_value, expected_profit, expected_roi）\\n\")\n\n    with open('evaluation_report.md','w',encoding='utf-8') as f:\n        f.write('\\n'.join(lines))\n\n    print(json.dumps({\n        'best_model': report['best_model'],\n        'auc': report['auc'],\n        'pr_auc': report['ap'],\n        'threshold': report['threshold'],\n        'precision': float(precision),\n        'recall': float(recall),\n        'paid_cost_used': used_cost,\n        'expected_profit': exp_profit,\n        'saved_files': ['model.pkl','feature_importance.png','roc_curve.png','pr_curve.png','confusion_matrix.png','churn_users.csv','evaluation_report.md']\n    }, ensure_ascii=False, indent=2))\n```\n",
    "特征提取SQL": "-- ClickHouse（行为与任务）按用户聚合近30天特征\n```sql\nWITH params AS (\n  SELECT today() AS run_date,\n         run_date - 30 AS obs_start,\n         run_date - 1 AS obs_end\n)\nSELECT\n  u.user_id,\n  toDate(max(ev.timestamp)) AS snapshot_date,\n  uniqExactIf(toDate(ev.timestamp), ev.event IN ('login','task_created','task_success')) AS active_days_30,\n  countIf(ev.event='login') AS login_cnt_30,\n  countIf(ev.event='task_created') AS tasks_created_30,\n  avgIf(ev.properties:processing_ms, ev.event='task_success') AS avg_proc_ms,\n  quantileExactHighIf(0.95)(ev.properties:processing_ms, ev.event='task_success') AS p95_proc_ms,\n  countIf(ev.event='task_success') / NULLIF(countIf(ev.event IN ('task_success','task_failed')),0) AS success_rate,\n  countIf(ev.event='task_failed') / NULLIF(countIf(ev.event IN ('task_success','task_failed')),0) AS fail_rate,\n  countIf(ev.event='payment_success') AS pay_cnt_30,\n  sumIf(ev.properties:amount, ev.event='payment_success') AS pay_amt_30,\n  sumIf(ev.properties:amount, ev.event='payment_success')/NULLIF(countIf(ev.event='payment_success'),0) AS aov,\n  dateDiff('day', maxIf(ev.timestamp, ev.event='payment_success'), max(ev.timestamp)) AS last_pay_gap,\n  countIf(ev.event='coupon_redeemed') AS coupon_used_cnt_30,\n  uniqExactIf(ev.properties:function_name, ev.event='feature_used') AS feature_variety,\n  sumIf(toDayOfWeek(ev.timestamp) IN (6,7), ev.event IN ('login','task_created','task_success')) / NULLIF(countIf(ev.event IN ('login','task_created','task_success')),0) AS weekend_share,\n  sumIf(toHour(ev.timestamp) BETWEEN 0 AND 6, ev.event IN ('login','task_created','task_success')) / NULLIF(countIf(ev.event IN ('login','task_created','task_success')),0) AS night_share,\n  avgIf(ev.properties:session_gap_hours, ev.event='login') AS avg_session_gap_h,\n  avgIf(ev.properties:quota_used/ev.properties:quota_total, ev.event='quota_snapshot') AS quota_use_ratio\nFROM events ev\nINNER JOIN users u ON ev.user_id = u.user_id\nCROSS JOIN params p\nWHERE toDate(ev.timestamp) BETWEEN p.obs_start AND p.obs_end\nGROUP BY u.user_id;\n```\n-- MySQL（属性数据）\n```sql\nSELECT user_id, register_time, channel, device_type AS device, country AS region\nFROM dim_user;\n```\n-- 标签构造（ClickHouse）\n```sql\nWITH win AS (\n  SELECT u.user_id, toDate(t) AS snapshot_date, t AS snap_ts\n  FROM (\n    SELECT arrayJoin(range(toUInt32(toDate('2025-01-01')), toUInt32(today()))) AS t\n  )\n  INNER JOIN users u\n)\nSELECT w.user_id, w.snapshot_date,\n       if(countIf(ev.user_id = w.user_id AND toDate(ev.timestamp) BETWEEN w.snapshot_date+1 AND w.snapshot_date+14 AND ev.event IN ('login','task_created','payment_success'))=0, 1, 0) AS label_churn_14d\nFROM win w\nLEFT JOIN events ev ON ev.user_id = w.user_id\nGROUP BY w.user_id, w.snapshot_date;\n```",
    "模型保存": "使用pickle保存：`model.pkl`（含模型+编码器+阈值），版本号以日期/commit标识，存储于对象存储（如S3/OSS）并写入元数据（特征版本、训练窗口、AUC）以便回溯；线上服务加载相同版本。"
  },
  "模型评估": {
    "门槛": "测试集 ROC-AUC ≥ 0.70；选择 Precision ≥ 0.60 的阈值用于召回；同时报告 PR-AUC、混淆矩阵、特征重要性Top20",
    "报告输出": "evaluation_report.md（含ROC、PR、CM、Top特征图），并记录分桶校准（可加入Brier score与Reliability曲线）",
    "注意": "严格时间切分；若线上分布漂移（KS/JSD）>阈值或AUC滑落>5%，触发重训/回滚"
  },
  "业务落地": {
    "召回分群": "高风险 p≥0.70；中风险 0.40≤p<0.70；低风险 p<0.40（仅用于策略初配）",
    "召回策略": {
      "高风险": "优惠券（默认成本5元，预期召回率30%）",
      "中风险": "短信提醒（0.05元/条，预期召回率10%）",
      "低风险": "Push（免费，预期召回率3%）"
    },
    "预算与ROI": "对每位用户 i，期望利润 E_i = p_i × r(seg_i) × V_i − cost(action_i)，其中 V_i≈α×近30天付费（α默认0.6保守）；在月预算5000元下，按 E_i/cost 从高到低选取（且E_i>0），Push不计入预算。生成 churn_users.csv（user_id, 流失概率, 分群, 策略, 预期成本, 期望利润, ROI）。",
    "上线": "将 `model.pkl` 部署为在线打分（每日批/实时），每日生成召回清单并推送到营销平台（短信、Push、券发放），并通过A/B验证召回策略实际提升。"
  },
  "持续优化": {
    "训练频率": "每月或当特征分布漂移(>3σ/KS>0.1)触发重训；新功能/新渠道上线后补充特征",
    "监控指标": "AUC/PR-AUC、Precision/Recall@业务阈值、分群转化率、单次触达ROI、召回带来的净GMV；线上/训练分布漂移（KS/JSD）",
    "策略迭代": "按分群动态配置券面值（高风险可5元券，轻度用户试2元券）、短信文案多版本多臂试验（支持ε-greedy/TS）",
    "合规与频控": "短信/Push频率上限、退订与隐私合规；黑名单与冷却期管理"
  },
  "输出物": [
    "model.pkl（模型文件）",
    "feature_importance.png（特征重要性Top20）",
    "roc_curve.png / pr_curve.png / confusion_matrix.png",
    "churn_users.csv（召回清单，含策略与预算）",
    "evaluation_report.md（评估与ROI汇总）"
  ],
  "交付给": "backend_dev_skill（落地离线/在线打分与清单生成；对接短信/Push/发券系统；Dashboard接入）"
}
````

---

````json
{
  "id": "DATA-SCIENCE-002",
  "title": "A/B 实验评估框架（样本量、显著性、CUPED、多指标与提前止损）",
  "objective": "提供稳定、低方差、可复用的实验评估工具集与报告模板，支撑产品与营销决策（PostHog 或自研实验框架对接）",
  "建模方案": {
    "核心能力": [
      "二项/连续指标检验：两比例Z检验（转化率）、Welch t检验（GMV/时长等）",
      "样本量计算：支持绝对/相对MDE、显著性水平α、检验功效1-β、组间比例",
      "CUPED：用协变量（预实验期同源指标）进行方差降低",
      "Bayesian A/B（Beta-Bernoulli）：给出 P(B>A)、后验均值与95%区间",
      "多指标决策：OEC（加权目标）+ Guardrail（守门指标）+ 分层/序贯控制",
      "提前止损：频率学派（O’Brien–Fleming/Pocock 型α花费近似）与贝叶斯（后验劣势阈值）"
    ],
    "统计方法选择": "转化率/点击率→两比例Z检验或贝叶斯Beta；均值类（GMV、时长）→Welch t；比例小样本用精确检验；分布偏态时可用bootstrap置信区间。",
    "决策规则": "上线门槛：主指标显著提升且守门指标不显著下降；若主指标提升但GMV下降，按OEC与利润敏感度权衡（见代码）。"
  },
  "技术实现": {
    "Python代码": "```python\n# -*- coding: utf-8 -*-\n\"\"\"\nA/B 实验评估框架（可直接运行）\n功能：样本量计算、两比例Z检验、t检验、CUPED、贝叶斯A/B、OEC与早停规则、自动生成报告\n输出：experiment_report.md\n依赖：pandas numpy scipy statsmodels matplotlib（若scipy/statsmodels不可用，将退化到近似计算）\n\"\"\"\nimport os, math, json, warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nfrom dataclasses import dataclass\n\n# 可选依赖\nHAVE_SCIPY = True\nHAVE_SM = True\ntry:\n    import scipy.stats as st\nexcept Exception:\n    HAVE_SCIPY = False\ntry:\n    from statsmodels.stats.power import NormalIndPower\n    from statsmodels.stats.proportion import proportions_ztest, proportion_confint\nexcept Exception:\n    HAVE_SM = False\n\n# ----------------------- 样本量计算 -----------------------\n@dataclass\nclass SampleSizeInput:\n    baseline: float  # 基线转化率 p0\n    mde: float       # 最小可检测提升（绝对值，如0.02表示+2个百分点）；若relative=True，则表示相对比例\n    alpha: float = 0.05\n    power: float = 0.80\n    ratio: float = 1.0  # 实验:对照 样本比\n    relative: bool = False\n\ndef z_value(p):\n    # 近似Z分位（two-sided alpha）\n    if HAVE_SCIPY:\n        return st.norm.ppf(p)\n    # 简易近似\n    lookup = {0.975:1.96, 0.95:1.645, 0.9:1.282, 0.8:0.842}\n    return lookup.get(p, 1.96)\n\ndef sample_size_two_prop(inp: SampleSizeInput):\n    p0 = inp.baseline\n    p1 = p0 * (1+inp.mde) if inp.relative else p0 + inp.mde\n    # 防御\n    p1 = max(min(p1, 0.999), 1e-6)\n    alpha = inp.alpha\n    power = inp.power\n    ratio = inp.ratio\n    # statsmodels 优先\n    if HAVE_SM:\n        from statsmodels.stats.power import NormalIndPower\n        from statsmodels.stats.proportion import proportion_effectsize\n        es = proportion_effectsize(p1=p1, p2=p0)\n        n1 = NormalIndPower().solve_power(effect_size=es, alpha=alpha, power=power, ratio=ratio, alternative='two-sided')\n        n2 = n1 * ratio\n        return math.ceil(n1), math.ceil(n2)\n    # 近似公式（等比例）\n    z_alpha = z_value(1-alpha/2)\n    z_beta = z_value(power)\n    pbar = (p0 + p1)/2\n    qbar = 1 - pbar\n    num = (z_alpha*math.sqrt(2*pbar*qbar) + z_beta*math.sqrt(p0*(1-p0) + p1*(1-p1)))**2\n    den = (p1 - p0)**2\n    n_equal = num/den\n    return math.ceil(n_equal), math.ceil(n_equal*ratio)\n\n# ----------------------- 两比例Z检验 / 置信区间 -----------------------\n\ndef two_prop_test(x1, n1, x2, n2, alpha=0.05):\n    # 返回：差值、p值、置信区间\n    p1, p2 = x1/n1, x2/n2\n    diff = p2 - p1  # B - A\n    if HAVE_SM:\n        stat, pval = proportions_ztest([x1, x2], [n1, n2])\n        # 正态近似置信区间\n        se = math.sqrt(p1*(1-p1)/n1 + p2*(1-p2)/n2)\n        z = z_value(1-alpha/2)\n        ci = (diff - z*se, diff + z*se)\n    else:\n        # 手算近似\n        p_pool = (x1+x2)/(n1+n2)\n        se_pool = math.sqrt(p_pool*(1-p_pool)*(1/n1+1/n2))\n        z = diff / (se_pool + 1e-12)\n        # 双侧p值\n        if HAVE_SCIPY:\n            pval = 2*(1-st.norm.cdf(abs(z)))\n        else:\n            # 正态近似\n            pval = 2*(1-0.5*(1+math.erf(abs(z)/math.sqrt(2))))\n        se = math.sqrt(p1*(1-p1)/n1 + p2*(1-p2)/n2)\n        zc = z_value(1-alpha/2)\n        ci = (diff - zc*se, diff + zc*se)\n    return diff, pval, ci\n\n# ----------------------- 连续指标 Welch t 检验 -----------------------\n\ndef welch_t_test(a, b, alpha=0.05):\n    a, b = np.array(a), np.array(b)\n    if HAVE_SCIPY:\n        t, p = st.ttest_ind(a, b, equal_var=False)\n    else:\n        # 简化t统计\n        ma, mb = a.mean(), b.mean()\n        va, vb = a.var(ddof=1), b.var(ddof=1)\n        na, nb = len(a), len(b)\n        se = math.sqrt(va/na + vb/nb)\n        t = (mb - ma) / (se + 1e-12)\n        # 自由度近似\n        df = (va/na + vb/nb)**2 / ((va*va)/(na*na*(na-1)) + (vb*vb)/(nb*nb*(nb-1)) + 1e-12)\n        # p 值近似（正态）\n        if HAVE_SCIPY:\n            p = 2*(1-st.t.cdf(abs(t), df))\n        else:\n            p = 2*(1-0.5*(1+math.erf(abs(t)/math.sqrt(2))))\n    return float(t), float(p)\n\n# ----------------------- CUPED -----------------------\n\ndef cuped_adjust(df, y_col, x_col, group_col):\n    # theta = Cov(Y, X) / Var(X)\n    x = df[x_col].values\n    y = df[y_col].values\n    theta = float(np.cov(y, x, ddof=1)[0,1] / (np.var(x, ddof=1) + 1e-12))\n    y_adj = y - theta * (x - np.mean(x))\n    df = df.copy()\n    df[y_col + '_adj'] = y_adj\n    # 对调整后的Y做检验（连续指标）\n    a = df[df[group_col]=='control'][y_col + '_adj']\n    b = df[df[group_col]=='treatment'][y_col + '_adj']\n    t, p = welch_t_test(a, b)\n    return df, theta, t, p\n\n# ----------------------- Bayesian Beta-Bernoulli -----------------------\n\ndef bayes_beta(a_succ, a_total, b_succ, b_total, prior_a=1, prior_b=1, ns=200000, seed=42):\n    rng = np.random.default_rng(seed)\n    post_a1, post_b1 = prior_a + a_succ, prior_b + (a_total - a_succ)\n    post_a2, post_b2 = prior_a + b_succ, prior_b + (b_total - b_succ)\n    a_samp = rng.beta(post_a1, post_b1, size=ns)\n    b_samp = rng.beta(post_a2, post_b2, size=ns)\n    prob_b_better = float(np.mean(b_samp > a_samp))\n    lift_samples = (b_samp - a_samp)\n    ci = (float(np.quantile(lift_samples, 0.025)), float(np.quantile(lift_samples, 0.975)))\n    return prob_b_better, ci, (post_a1, post_b1, post_a2, post_b2)\n\n# ----------------------- OEC & 决策 -----------------------\n\ndef decide_with_oec(conv_diff, gmv_diff, weights=(1.0, 0.5), guardrail_min_gmv=-1e-9):\n    # conv_diff/ gmv_diff 为 B-A 的绝对差\n    oec = weights[0]*conv_diff + weights[1]*(gmv_diff)\n    decision = 'ship' if (oec>0 and gmv_diff>=guardrail_min_gmv) else 'hold'\n    return float(oec), decision\n\n# ----------------------- 早停规则 -----------------------\n\ndef obrien_fleming_alpha(total_alpha, k_looks, i_look):\n    # 近似：alpha_i = 2*(1-Φ(z*/sqrt(i/k))) with boundary z*~z_{1-alpha/2}\n    if HAVE_SCIPY:\n        zstar = st.norm.ppf(1-total_alpha/2)\n        frac = (i_look/ k_looks)\n        ai = 2*(1-st.norm.cdf(zstar/ math.sqrt(max(frac, 1e-6))))\n    else:\n        zstar = 1.96\n        frac = (i_look/ k_looks)\n        ai = 2*(1-0.5*(1+math.erf((zstar/ math.sqrt(max(frac, 1e-6)))/math.sqrt(2))))\n    return max(min(ai, total_alpha), 1e-6)\n\n# ----------------------- Demo / 报告 -----------------------\nif __name__ == '__main__':\n    # 1) 样本量计算示例：基线5%，想检测 +2 个百分点（到7%），80%功效，α=0.05\n    ss = SampleSizeInput(baseline=0.05, mde=0.02, alpha=0.05, power=0.80, ratio=1.0, relative=False)\n    n_ctrl, n_trt = sample_size_two_prop(ss)\n\n    # 2) 构造一个示例实验（可替换为真实明细数据）\n    rng = np.random.default_rng(42)\n    n1, n2 = n_ctrl, n_trt\n    pA, pB = 0.05, 0.07\n    a = rng.binomial(1, pA, size=n1)\n    b = rng.binomial(1, pB, size=n2)\n    x1, x2 = int(a.sum()), int(b.sum())\n\n    # 二项检验\n    diff, pval, ci = two_prop_test(x1, n1, x2, n2)\n\n    # 连续指标（例如 GMV 人均）：模拟\n    gmv_A = rng.gamma(2.0, 20.0, size=n1)\n    gmv_B = rng.gamma(2.1, 20.0, size=n2)  # 略有提升\n    t_stat, p_t = welch_t_test(gmv_A, gmv_B)\n\n    # CUPED（以 pre_GMV 为协变量）\n    df_demo = pd.DataFrame({\n        'group': ['control']*n1 + ['treatment']*n2,\n        'gmv': np.concatenate([gmv_A, gmv_B]),\n        'pre_gmv': rng.gamma(2.0, 18.0, size=n1+n2)\n    })\n    df_adj, theta, t_adj, p_adj = cuped_adjust(df_demo, 'gmv', 'pre_gmv', 'group')\n\n    # Bayes A/B（转化率）\n    pb, ci_bayes, post = bayes_beta(x1, n1, x2, n2)\n\n    # OEC 决策（示例：权重= [转化1.0, GMV 0.5]，守门：GMV不为负）\n    conv_diff = (x2/n2) - (x1/n1)\n    gmv_diff = df_adj[df_adj['group']=='treatment']['gmv_adj'].mean() - df_adj[df_adj['group']=='control']['gmv_adj'].mean()\n    oec, decision = decide_with_oec(conv_diff, gmv_diff, weights=(1.0, 0.5), guardrail_min_gmv=0.0)\n\n    # 早停门槛演示：假设计划看5次，这是第3次\n    alpha3 = obrien_fleming_alpha(0.05, 5, 3)\n\n    # 报告\n    lines = []\n    lines.append('# 实验评估报告\\n')\n    lines.append('## 样本量计算\\n')\n    lines.append(f\"基线=5%，MDE=+2个百分点（绝对），α=0.05，功效=0.80 ⇒ **每组所需样本约 {n_ctrl:,}** 人。\\n\")\n    lines.append('（若MDE为相对+2%，则每组需要 ~75万，极不现实，应放宽时长或放大MDE）\\n')\n\n    lines.append('\\n## 频率学派检验\\n')\n    lines.append(f\"转化率差(B-A)={diff:.4f}，p值={pval:.4g}，95%CI=({ci[0]:.4f}, {ci[1]:.4f})\\n\")\n    lines.append(f\"GMV Welch t 检验：t={t_stat:.3f}, p={p_t:.4g}（未用CUPED）\\n\")\n\n    lines.append('\\n## CUPED（方差降低）\\n')\n    lines.append(f\"θ={theta:.3f}；调整后GMV Welch t 检验 p={p_adj:.4g}（通常p下降、置信区间变窄）\\n\")\n\n    lines.append('\\n## 贝叶斯A/B（转化率）\\n')\n    lines.append(f\"P(B>A)={pb:.3f}；95%后验区间（lift）=({ci_bayes[0]:.4f}, {ci_bayes[1]:.4f})\\n\")\n\n    lines.append('\\n## 多指标决策（OEC + Guardrail）\\n')\n    lines.append(f\"conv_diff={conv_diff:.4f}, gmv_diff={gmv_diff:.2f} ⇒ OEC={oec:.4f} ⇒ 建议：**{decision}**\\n\")\n\n    lines.append('\\n## 提前止损/早停\\n')\n    lines.append(f\"O’Brien–Fleming 第3/5次观测，当前显著性阈值≈{alpha3:.4f}；若 p < {alpha3:.4f} 可提前判优；若 P(B>A)<0.05（贝叶斯）或条件功效<20% 则可判劣提前止损。\\n\")\n\n    with open('experiment_report.md','w',encoding='utf-8') as f:\n        f.write('\\n'.join(lines))\n\n    print(json.dumps({\n        'sample_size_each_group': n_ctrl,\n        'two_prop_p_value': float(pval),\n        'bayes_prob_B_better': float(pb),\n        'oec': float(oec),\n        'decision': decision,\n        'saved_files': ['experiment_report.md']\n    }, ensure_ascii=False, indent=2))\n```\n",
    "样本量计算器": "见上述 `sample_size_two_prop()` 与 `SampleSizeInput`；示例：基线5%、MDE=+2个百分点、功效80%、α=0.05 ⇒ 每组约2213人；功效90% ⇒ 每组约2961人；若MDE为相对+2%（5%→5.1%）需每组≈752,702人，不现实。",
    "实验报告模板": "代码会生成 experiment_report.md，包含：样本量、频率学派检验结果（差值、p、置信区间）、CUPED参数与p值、贝叶斯概率P(B>A)与区间、OEC与决策、早停门槛。",
    "实验平台集成方案": {
      "事件采集": "在曝光时打点 `experiment_exposure`，属性包含 {experiment: 'new_user_coupon', variant: 'control|treatment', exp_version, feature_flag_id}；关键转化如 `signup_ok`, `purchase_success`。",
      "与PostHog对接（ClickHouse）": "推荐将 exposure 与 conversion 事件写入 PostHog/ClickHouse，按 distinct_id 关联。SQL示例见下："
    },
    "数据模型SQL": "-- 曝光样本（最近14天）\n```sql\nWITH base AS (\n  SELECT distinct_id, min(toDate(timestamp)) AS first_exposure_date,\n         argMin(properties['variant'], timestamp) AS variant\n  FROM events\n  WHERE event = 'experiment_exposure'\n    AND properties['experiment'] = 'new_user_coupon'\n    AND toDate(timestamp) >= today()-14\n  GROUP BY distinct_id\n)\nSELECT b.distinct_id, b.variant,\n       sumIf(event='purchase_success') AS purchase_cnt,\n       countIf(event='purchase_success') AS conversions,\n       count() AS total_events\nFROM events e\nJOIN base b USING (distinct_id)\nWHERE toDate(e.timestamp) BETWEEN b.first_exposure_date AND b.first_exposure_date + 7\nGROUP BY b.distinct_id, b.variant;\n```\n-- 指标聚合（转化率、GMV）\n```sql\nSELECT variant,\n       countDistinct(distinct_id) AS users,\n       countIf(event='purchase_success') AS purchasers,\n       sumIf(properties['amount'], event='purchase_success') AS gmv\nFROM events e\nWHERE event IN ('experiment_exposure','purchase_success')\n  AND toDate(timestamp) >= today()-14\nGROUP BY variant;\n```"
  },
  "模型评估": {
    "样本量示例": "基线5%，MDE=+2个百分点（绝对），α=0.05、功效80% ⇒ **每组≈2213**；功效90% ⇒ **≈2961**。若MDE为相对+2% ⇒ 每组≈752,702（不建议）。",
    "显著性检验": "主度量：两比例Z检验/贝叶斯；连续度量：Welch t；报告95%置信区间与效应量（Cohen's h/d）。",
    "CUPED": "使用 pre-period 的同源指标（如 pre_GMV、pre_session）校正，写入报告θ系数与p值变化。",
    "提前结束标准": [
      "判优：在第i次观测时 p < α_i（O’Brien–Fleming α花费），或贝叶斯 P(B>A)≥0.99",
      "判劣：条件功效<20% 或 贝叶斯 P(B>A)≤0.05，或守门指标（如GMV）95%CI上界<0"
    ],
    "多指标权衡": "设定OEC权重（如转化1.0、GMV 0.5），若主指标显著但GMV下降，依据OEC与守门阈值（GMV不为负）综合决策；必要时延伸实验以获取更高功效。"
  },
  "业务落地": {
    "流程": "产品/营销在需求单中定义：实验名称、版本、曝光点、主指标、守门指标、最短时长与最大观测次数；研发接入曝光与转化打点；日更评估并写入仓库。",
    "报告": "统一生成 experiment_report.md；落地看板展示：样本进度、功效达成度、主/辅指标差值与CI、CUPED后方差变化、贝叶斯P(B>A)、早停阈值与结果。",
    "上线建议": "仅当主指标显著提升、守门指标不劣、OEC>0 且稳定2+天后上线；若边缘显著，延续实验直至功效达成或触发早停。"
  },
  "持续优化": {
    "指标治理": "统一指标口径（用户层 vs 事件层）、去重与异常过滤（机器人、退款负值），建立度量字典；",
    "低方差实践": "充分使用CUPED/分层（渠道、地域、设备）；避免漂移期间混入新渠道；",
    "并发实验": "采用分层/互斥分流避免干扰；多臂/TS 适用于文案与券面值动态分配；",
    "审计": "对PostHog/数据管道做采样校验（曝光->转化闭环率），异常自动报警。"
  },
  "输出物": [
    "experiment_report.md（标准化实验结论）",
    "sample_size函数/脚本（可嵌入平台）",
    "统计检验与CUPED库（复用到每个实验管线）",
    "集成SQL（ClickHouse/PostHog）与仪表板定义"
  ],
  "交付给": "data_platform_engineer（集成到实验平台与看板，沉淀为可复用服务）"
}
````

---

## 备注与使用指南

* **直接运行**：将两段 Python 脚本分别保存并执行，即会在当前目录生成评估报告、图表、模型与召回清单（使用合成数据）。替换成真实数据时：

  1. 用给出的 ClickHouse/MySQL SQL 导出到 CSV（或在脚本中直接查询），
  2. 对齐字段名（与代码中的 `num_cols/cat_cols` 一致），
  3. 调整 `CFG`（预算、成本、召回率、α 等），即可无缝落地。
* **指标门槛**：流失模型须 **AUC≥0.70**；召回阈值在 **Precision≥0.60** 前提下由预算与 ROI 决定。
* **ROI 回看**：在召回上线后，用实验框架对“召回 vs 不召回”进行 A/B 或倾向分数匹配（PSM）评估真实增益（而非仅相关性）。
* **看板建议**（两任务共用）：

  * 训练与线上 AUC/PR-AUC；\n  - 分群规模、成本占用、实际召回率与净GMV；\n  - 样本量达成进度、实时 p 值/CI、贝叶斯概率、CUPED 方差降低比例；\n  - 异常告警（数据缺口/打点掉线/漂移）。

如需，我也可以把这两段脚本合并为一个可配置的 CLI 工具，或按你现有的数据表结构改写 SQL 与字段映射。
